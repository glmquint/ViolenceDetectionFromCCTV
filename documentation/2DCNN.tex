\chapter{2D CNN approach}
The very fist approach we tried was to develop from scratch a 2D CNN to recognize the violence in the videos, to do that we needed to extract the frames from the videos and feed it to the AI model since a 2D CNN processes images.
\section{Frame extraction}
The idea initially was to do it with the help of \textit{FFmpeg}. FFmpeg is a comprehensive software suite for recording, converting, and playing audio and video, it relies on libavcodec, a library for audio/video encoding. Following there is an example of usage.
\begin{lstlisting}[language=bash, caption={FFmpeg example}, label={lst:FFmpegExample}]
#!/bin/bash
# To force the frame rate of the output file to 24 fps:
ffmpeg -i input.avi -r 24 output.avi

\end{lstlisting}

Following there is the code used to extract the frames from the videos. For brevity we will show only the code used to extract the frames from the "fight" directory and the paths are removed.
\begin{lstlisting}[language=bash, caption={Frame extraction}, label={lst:FrameExtraction}]
for f in $(ls $path_to_video); do 
	ffmpeg -i $path_to_video/$f -vf fps=5 $path_to_frame/${f%.*}-%03d.png; 
done;
\end{lstlisting}

This command uses FFmpeg to extract frames from a video file and save them as individual images, it iterates over each file in the specified directory and applies the FFmpeg command to convert the video to frames. The frames are saved with the same name as the original video file, followed by a three-digit number to indicate the frame sequence, the frames are saved in the specified output directory.
However, this method was found to not be flexible enough, in the light of new approach to the problem like a 3D CNN, more in the next chapter, so we decided to use a different approach, we decided extract the frames at runtime and feed them to the AI model with differences depending on the model and type of dataset fed to it. However performing the frame extraction each time we wanted to train a network would have been too much of a workload. We decided instead to separate the extraction phase from the training phase and save a serialized copy of the preprocessed dataset on files, through the help of the \textit{pickle} library in Python. The following code shows the basic idea of the frame extraction process:

\begin{lstlisting}[language=python, caption={Pickles generation}, label={lst:PicklesGeneration}]
    # Function to load a dataset of videos with corresponding labels
   function load_video_dataset(dataset_path, pkl_config, create_on_missing):
        # Initialize empty lists for training and testing data and labels
        train_data = []
        train_labels = []
        test_data = []
        test_labels = []
    
        # Generate paths for storing pickle files
        pickles_dir =  f'{ROOT_PATH}/pickles'
        pickle_name = sha256([pickle key information])
    
        # Create a list of pickle file paths
        # f'{pickles_dir}/{pickle_name}-train_data.pkl'
        # f'{pickles_dir}/{pickle_name}-train_labels.pkl'
        # f'{pickles_dir}/{pickle_name}-test_data.pkl'
        # f'{pickles_dir}/{pickle_name}-test_labels.pkl'
        try:
            # Attempt to load data and labels from pickle files
            load_data_from_pickles([pickle_file_paths...])
    
        except FileNotFoundError:
            if create_on_missing:
                # Create a list of labeled video paths from the dataset
                labelled_paths = get_labeled_video_paths(dataset_path)
    
                # Shuffle the list of labeled video paths
                # Avoid deleting always the same videos at balancing time
                shuffle(labelled_video_paths)
    
                # Load and preprocess videos,balancing
                # the number of samples for each class
                labelled_frame = balance(labelled_paths, pkl_config)
    
                # Split the data into training and testing sets
                split_data(labelled_frame, pkl_config, min_amount)
    
                # Save the data and labels as pickle files
                save_pickles([pickle_file_paths...])
    
                # Update the pickle register with 
                #the new pickle file information
                update_pickle_register(pickles_dir, pickle_name, pkl_config)
    
        except Exception as e:
            print(f"An unexpected error occurred: {e}")
    
        # Return the loaded or newly created data and labels
        return train_data, train_labels, test_data, test_labels
    

\end{lstlisting}
The main idea is to create 4 pickle files for each configuration (train\_data, train\_label, test\_data, test\_label), the pickle files are created only if they are not present, this way we can load them at runtime and avoid the frame extraction process. In case of creation needed, the videos are shuffled before the extraction phase. In this way, after the balancing process to obtain the same number of samples from the \textit{violence} and \textit{non violence} categories, we avoid removing only certain videos coming from a specific source. This process is flexible enough to allow us to create pickles for both 2D CNNs and 3D CNNs too. In the 3D case, the samples consist in a burst of consecutive frames. The \textit{pkl\_config} parameters contains a dictionary with the following information:
\begin{itemize}
	\item frame size
	\item number of frames
	\item train split
	\item fps
	\item crop
\end{itemize}
The \textit{frame size} specifies the height and width dimensions of the squared frame. The \textit{number of frames} is the number of frames to be extracted from the video for each \textit{burst}. The \textit{train split} is the fractional split between training and testing. The \textit{fps} is the frame per second of the video, for example if we want to train a 2D CNN both the \textit{number of frames} and the \textit{fps} will be 1, if we want to train a 3D CNN with burst of 2 seconds we might set the \textit{number of frames} to 10 and the \textit{fps} to 5. The \textit{crop} is a boolean value that indicates if the frame should be cropped or not to remove the black bars, as explained in section \ref{sec:datasetcleaning}.

\section{Dataset cleaning}
\label{sec:datasetcleaning}
The dataset was very dirty. This fact resulted in many difficulties throughout the project development: the dataset was composed of videos of different lengths, different resolutions and different frame rates. Furthermore, many videos contained black bars on the sides, making many AI models learn from them instead of the actual features. To solve this problem we decided to crop the videos removing black bars, this was done with the following code:

\begin{lstlisting}[language=Python, caption={Image Cropper}, label={lst:ImageCropper}]
def crop(image, y_nonzero, x_nonzero):
    # If y_nonzero and x_nonzero are not provided, calculate them from the grayscale version of the image
    if y_nonzero is None or x_nonzero is None:
        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        y_nonzero, x_nonzero = np.where(gray_image > 1)

    # Crop the image based on the calculated non-zero values
    cropped_image = image[np.min(y_nonzero):np.max(y_nonzero), np.min(x_nonzero):np.max(x_nonzero)]

    # Return the cropped image along with updated y_nonzero and x_nonzero values
    return cropped_image, y_nonzero, x_nonzero
\end{lstlisting}

However this proved to be not enough, the dataset was still very noisy: elements like writings over the videos, some rare mislabeled frames and other problems increased the difficulty for models to learn the correct features. An initial approach for frame cropping involved a simple cut-out of the largest square centered in the frame to prevent distortion, but this resulted in the exclusion of many areas of action for all those videos containing violence on the edge of the screen. The final approach introduces a light distortion with respect to the original source, but we preserve all relevant information, discarding all rows and columns of pixels containing pure black values. For future works on the subject, it would be ideal to aggregate a much more comprehensive dataset, which would include bounding boxes to better identify the actual areas of action.

\section{First model}
For the first model we decided to use a very simple 2D CNN, the model was composed of 2 convolutional layers, 2 max pooling layers and 1 dense layer, the model was trained for 100 epochs with a batch size of 32 samples. To avoid overfitting, we used the Early Stopping technique with patience set to 15, after an initial warm-up period of 5 epochs; we also set the \textit{restore best weight} flag to save the best model before the validation loss increase. The optimizer used was Adam. This parameters were found to be performing best from early experimentation runs. The model was trained on the dataset with the black bars removed, the dataset was composed of 2 classes, violence and no-violence, the model was trained on 80\% of the dataset and tested on the remaining 20\%, the validation set was 30\% of the training set with a hold out methodology.  

\begin{figure}[]
    \centering
    \includegraphics[width=0.8\textwidth, height=0.6\textwidth, keepaspectratio]{images/simple3.png}
    \caption{Initial 2D CNN model}
    \label{fig:First2DCNN}
\end{figure}

\begin{figure}[]
    \centering
    \includegraphics[width=1\textwidth]{images/731a-simple3-b538-history.png}
    \caption{Training history of the first model}
    \label{fig:First2DCNNHistory}
\end{figure}

\begin{figure}[]
    \centering
    \includegraphics[width=0.8\textwidth]{images/731a-simple3-b538-conf_matrix.png}
    \caption{Confusion matrix of the first model}
    \label{fig:First2DCNNConfusionMatrix}
\end{figure}


The first comment to be made is that the confusion matrix is heavily bias towards the \textit{Non violence} class. This is due to the fact that the dataset can be quite chaotic and the model is not able to learn the features of the videos. For example the model may learn form the Non violence part of the dataset that gatherings of people are not to be flagged as violence, however this could easily lead to many false negatives (Violence images classified as Non violence ones).

Another comment is to be made regarding the validation loss and accuracy, which is quite low in both cases, this could signal overfitting and general failing in extrapolating the main features of the videos. Since the training accuracy is almost immediately we can assume that the dataset is too small and the model starts to overfit in the first epochs.
\pagebreak
\section{Second model - data augmentation}
For the second model we decided to keep the convolution layers to 2, add a dense layer and, most importantly, to add random\_flip(horizontal) and random\_rotation(0.1)  as shown in Fig. \ref{fig:Second2DCNN} to make it harder for the model to overfit immediately. All other parameters remained unchanged: this was done to see if the model would have been able to produce better results. 
We decided to use a cautious approach since the problem presented itself as a very complicated one and we did not want to aport to many changes in a single \textit{pass} to avoid a time consuming trial and error process. 

Speaking of results the model no longer saturates training accuracy immediately, this could be a sign of a better generalization capability, however the model still has a very low validation accuracy meaning that the model is still not able to generalize well. The confusion matrix is shown in Fig. \ref{fig:Second2DCNNMatrix}, the model is still heavily biased towards the \textit{Non violence} class, this is due to the fact that the dataset is very noisy and the model is not able to learn the features of the videos.

\begin{figure}[]
    \centering
    \includegraphics[width=0.8\textwidth, height=0.6\textwidth, keepaspectratio]{images/simple4augnozoom.png}
    \caption{Second 2D CNN model}
    \label{fig:Second2DCNN}
\end{figure}


\begin{figure}[]
    \centering
    \includegraphics[width=1\textwidth]{images/731a-simple4augnozoom-b538-history.png}
    \caption{Training history of the second model}
    \label{fig:Second2DCNNHistory}
\end{figure}


\begin{figure}[]
    \centering
    \includegraphics[width=0.8\textwidth]{images/731a-simple4augnozoom-b538-conf_matrix.png}
    \caption{Confusion matrix of the second model}
    \label{fig:Second2DCNNMatrix}
\end{figure}

\pagebreak
\section{Third model - \textit{upgrading} the model}
Based on the results of the previous models we decided to try to \textit{upgrade} the model, this was done for two main reasons. The first one was to test the behavior of it in case of a more capable model. The second one was that, since the last results were not satisfactory especially in the validation accuracy, we thought the model were possibly overfitting, but it was doing it on the wrong features possibly like background, writings and other things. So we decide to amp the model capabilities as show in Fig. \ref{fig:Third2DCNN}.
\begin{figure}[]
    \centering
    \includegraphics[width=0.8\textwidth, height=0.8\textwidth, keepaspectratio]{images/simple3augConv8_64Dense128_64nozoom.png}
    \caption{Third 2D CNN model}
    \label{fig:Third2DCNN}
\end{figure}

Moving to the results of the model we can see in Fig. \ref{fig:Third2DCNNMatrix} that the confusion matrix is now more balanced than the previous ones, with an accuracy rate of almost 60\%. However the recall value are still not good enough, especially the violence one with a score of 57\%. Another thing to notice is the training history graph in Fig. \ref{fig:Third2DCNNHistory}, where the validation loss and accuracy are very unstable. This lead us to the conclusion that the model is now overfitting on the right features, so the next logical step is to try to fight it with dropout layers.
\begin{figure}[]
    \centering
    \includegraphics[width=1\textwidth]{images/731a-simple3augConv8_64Dense128_64nozoom-b538-history.png}
    \caption{Training history of the third model}
    \label{fig:Third2DCNNHistory}
\end{figure}
\begin{figure}[]
    \centering
    \includegraphics[width=0.8\textwidth]{images/731a-simple3augConv8_64Dense128_64nozoom-b538-conf_matrix.png}
    \caption{Confusion matrix of the third model}
    \label{fig:Third2DCNNMatrix}
\end{figure}
\pagebreak
\section{Fifth model}
Instead of further complicating the model we decided to simplify it to avoid the phenomenon of overfitting. The fifth model is the simplest one and has decent result, with a good recall but produces a large number of false positive as shown in  Fig. \ref{fig:FifthModelConfMatrix}. Even if this model leads to a higher accuracy and Violence recall than the previous ones, the poor Non violence recall score is too low to be considered a good model. In Tab. \ref{tab:2DCNNTable} there is a recap of the accuracy and recall of the models. The code of the fifth model is shown in the following listing: 

\begin{lstlisting}[language=python, caption={Fifth model}, label={lst:Fifth model}]
model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
\end{lstlisting}
\begin{table}[]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{c|c|c|c|}
    \cline{2-4}
                                                   & \textbf{Accuracy} & \textbf{Violence recall} & \textbf{Non violence recall} \\ \hline
    \multicolumn{1}{|c|}{\textbf{First Model}}     & 0,6095            & 0,3238                   & 0,8952                       \\ \hline
    \multicolumn{1}{|c|}{\textbf{Second Model}}    & 0,6238            & 0,5762                   & 0,6714                       \\ \hline
    \multicolumn{1}{|c|}{\textbf{Third Model}}     & 0,55              & 0,1095                   & 0,9905                       \\ \hline
    \multicolumn{1}{|c|}{\textbf{Final model}} & 0,7048            & 0,9095                   & 0,5                          \\ \hline
    \end{tabular}%
    }
    \caption{2D model accuracy and recall}
    \label{tab:2DCNNTable}
\end{table}

\begin{figure}[]
    \centering
    \includegraphics[width=0.8\textwidth]{images/731a-2D10_inv-86ad-conf_matrix.png}
    \caption{Fifth model}
    \label{fig:FifthModelConfMatrix}
\end{figure}

\section{Pretrained models: ResNet50}
Since the 2D \textit{from scratch} CNN approach was not working we decided to try a different method, we decided to use pretrained models. This was done to have a better understanding of the problem by comparing the results of them with the ones we presented previously. We decided to use Resnet50 as the first pretrained model. \\

ResNet50 \footnote{\url{https://arxiv.org/abs/1512.03385}} was developed by Microsoft Research in 2015, it features a deep structure with 50 layers, utilizing residual blocks that allow the network to learn residual functions to ease the optimization process.
The core innovation lies in skip connections, where the input from one layer is added to the output of another, facilitating the flow of gradients during backpropagation.
ResNet50  won the ImageNet Large Scale Visual Recognition Challenge in 2015. It includes bottleneck building blocks to improve computational efficiency by reducing the number of parameters in the intermediate layers.
\begin{figure}[]
    \centering
    \includegraphics[width=0.8\textwidth]{images/ResNet50_architecture.png}
    \caption{ResNet50 architecture}
    \label{fig:ResNet50Arch}
\end{figure}
We used ResNet50 with the imagenet weights, and we choose to do 
\textit{fine tuning}. We started by adding our dense layers for classifications and we only train the last convolutional block.  

\begin{lstlisting}[language=python, caption={ResNet50}, label={lst:resnet50Code}]
    resnet = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)
    set_trainable = False
    for layer in resnet.layers:
        if layer.name == 'conv5_block1_1_conv':
            set_trainable = True
        if set_trainable:
            layer.trainable = True
        else:
            layer.trainable = False
    model.add(resnet)
    model.add(GlobalAveragePooling2D())
    model.add(Dense(256, activation='relu'))
    model.add(Dense(128, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
\end{lstlisting}

The model shows a promising results, with a good confusion matrix and also with a 76\% recall on violence

\begin{figure}[]
    \centering
    \includegraphics[width=0.8\textwidth]{images/731a-resnet2D1-86ad-conf_matrix.png}
    \caption{ResNet50 first model}
    \label{fig:ResNet2d1}
\end{figure}

We continued to improve the model adding the layers for data augmentation, however we noticed worse performances compared to the first model and signals of overfitting. 
\begin{lstlisting}[language=python, caption={ResNet50 Second Test}, label={lst:resnet50CodeSecondTest}]
    resnet = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)
    set_trainable = False
    for layer in resnet.layers:
        if layer.name == 'conv5_block1_1_conv':
            set_trainable = True
        if set_trainable:
            layer.trainable = True
        else:
            layer.trainable = False

    model.add(RandomFlip('horizontal'))
    model.add(RandomRotation(0.1))
    model.add(RandomZoom(0.2))
    model.add(resnet)
    model.add(GlobalAveragePooling2D())
    model.add(Dense(256, activation='relu'))
    model.add(Dense(128, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
\end{lstlisting}

In order to fight overfitting we introduced a dropout layer and the overall accuracy improved, and we obtained better results compared to the original model. 
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{images/ROCResnet.png}
    \caption{ROC of the first Resnet50 vs third Resnet50 model}
    \label{fig:ROCResnet}
\end{figure}

\begin{lstlisting}[language=python, caption={ResNet50 Third test}, label={lst:resnet50CodeThirdTest}]
    resnet = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)
    set_trainable = False
    for layer in resnet.layers:
        if layer.name == 'conv5_block1_1_conv':
            set_trainable = True
        if set_trainable:
            layer.trainable = True
        else:
            layer.trainable = False

    model.add(RandomFlip('horizontal'))
    model.add(RandomRotation(0.1))
    model.add(RandomZoom(0.2))
    model.add(resnet)
    model.add(GlobalAveragePooling2D())
    model.add(Dense(256, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(128, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
\end{lstlisting}


We also run a fourth test applying a small dropout also to the second dense layer but we not obtain better results compared to the previous model. We assume that our network needs the 128 neurons for a correct classification of the problem. 
\begin{lstlisting}[language=python, caption={ResNet50 Fourth test}, label={lst:resnet50CodeFourthTest}]
    resnet = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)
    set_trainable = False
    for layer in resnet.layers:
        if layer.name == 'conv5_block1_1_conv':
            set_trainable = True
        if set_trainable:
            layer.trainable = True
        else:
            layer.trainable = False

    model.add(RandomFlip('horizontal'))
    model.add(RandomRotation(0.1))
    model.add(RandomZoom(0.2))
    model.add(resnet)
    model.add(GlobalAveragePooling2D())
    model.add(Dense(256, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.1))
    model.add(Dense(1, activation='sigmoid'))
\end{lstlisting}


Following a recap of the accuracy of the models and the recall:

\begin{table}[!h]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{c|c|c|c|}
    \cline{2-4}
                                                    & \textbf{Accuracy} & \textbf{Violence recall} & \textbf{Non Violence recall} \\ \hline
    \multicolumn{1}{|c|}{\textbf{ResNet 1st Model}} & 0,7429            & 0,7667                   & 0,7191                       \\ \hline
    \multicolumn{1}{|c|}{\textbf{ResNet 2nd Model}} & 0,6357            & 0,3619                   & 0,9095                       \\ \hline
    \multicolumn{1}{|c|}{\textbf{ResNet 3rd Model}} & 0,7643            & 0,6952                   & 0,8333                       \\ \hline
    \multicolumn{1}{|c|}{\textbf{ResNet 4th Model}} & 0,6809            & 0,6571                   & 0,7048                       \\ \hline
    \end{tabular}%
    }
    \caption{ResNet50 accuracy and recall}
    \label{tab:ResNet50}
    \end{table}

As can be seen in Tab. \ref{tab:ResNet50} the best model is the third one, this is due to the fact that the dataset is very small and the model is not able to learn from it, so the data augmentation is a must. The fourth model is not as good as the third one, because of the dropout layers which are too many and the model is not able to learn features properly. The first model is the second best and function as a base line. The second one is the worst of the collection, this is due to the fact that the data augmentation alone is not enough and also leads to overfitting of non real features as background or object in the videos.

\section{Pretrained models: EfficientNetB0}
EfficientNetB0\footnote{\url{https://arxiv.org/abs/1905.11946}} is part of the EfficientNet family, designed to achieve superior performance with fewer parameters compared to traditional models.
It introduces a compound scaling method that uniformly scales the depth, width, and resolution of the network, leading to improved efficiency across all dimensions.
The architecture includes mobile inverted bottleneck blocks and squeeze-and-excitation blocks to enhance feature extraction and model expressiveness.

EfficientNetB0 is also suitable for resource-constrained environments like mobile devices, being computationally efficient. 
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{images/EfficientNetB0-architecture-36.png}
    \caption{EfficientNetB0 architecture}
    \label{fig:EfficientNetB0architecture}
\end{figure}

The reason we choose EfficientNetB0 is because it can be used in IoT devices, like cameras, for a first filter on images for police operators.
Instead of going directly to fine tuning we start with a \textit{features extraction} approach.
We start from the simplest implementation as possible, with only a final dense layer with one neuron with a sigmoid activation function.
We obtained a 62\% overall accuracy, comparable to the second model of the \emph{scratch} 2D CNN but below both ResNet and the final model. In order to improve the performance of the network we add data augmentation layers and also more dense layers.
The accuracy increased from 62\% to 69\% and accordingly the violence recall (48\% compared to 38\%), however we are still far away from an acceptable level. This leads us to the conclusion that the best approach to get good results is to go to fine tuning. 
We decided to train only the two last convolutional blocks, since going at the first layers will need a huge dataset, we leave the same dense layers for classification as used in fine tuning. 
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{images/731a-EfficientNetB0_15-5a4e-conf_matrix.png}
    \caption{EfficientNetB0: third model}
    \label{fig:EfficientNetB0_15}
\end{figure}
The results obtained are better than the ones in fine tuning, this models also obtains obtains similar results in terms of accuracy and violence recall, to the third ResNet model. 
We follow this approach unfreezing another half of the fifth convolutional blocks.
Looking at the ROC curves we cannot draw conclusions on the winner but the AUC is better and also between 0.1 and 0.8 false positive rate the fourth model performs better than the third. 

However we try to push the model to the limit adding another dense layer in order to improve the classifier performance and adding also dropout to avoid overfitting.
We obtain a good model, with a 78\% accuracy with a strong recall on violence, that we used as a second choice parameter. 
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{images/731a-EfficientNetB0_24-b2b1-conf_matrix.png}
    \caption{EfficientNetB0: fifth model}
    \label{fig:EfficientNetB0_24}
\end{figure}

\begin{table}[]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|l|l|}
\hline
                               & \textbf{Accuracy} & \textbf{Violence recall} & \textbf{Non Violence recall} \\ \hline
\textbf{Features extraction 1} & 0,6214            & 0,3857                   & 0,85714                      \\ \hline
\textbf{Features extraction 2} & 0,6928            & 0,4809                   & 0,90476                      \\ \hline
\textbf{Fine tuning 1}         & 0,7690            & 0,70952                  & 0,82857                      \\ \hline
\textbf{Fine tuning 2}         & 0,7762            & 0,7191                   & 0,8333                       \\ \hline
\textbf{Fine tuning 3}         & 0,7833            & 0,8762                   & 0,6905                       \\ \hline
\end{tabular}%
}
\caption{EfficientNetB0 accuracy and recall}
\label{tab:EfficientNetB0 }
\end{table}
\section{Reamining problems and solutions} 
The ResNet50 and EfficientNetB0 models brought some improvements to the accuracy of the model in comparison to the ones we developed, however we thought we could have done better, the main issue, as said before, is the dataset, it is too dirty and it has no action frames or bounding boxes to help the model learn the features of the videos. This leads, during the frame extract phase, to images before the violent acts or after them to be fed to the model with a violence label, this makes the model learns the wrong features. To solve this problem we could have discarded the \textit{bad} frames, but this would need to manually remove them, which would have been too much of a workload and would have been outside of the scope of the project.

    What the 2D models lack is context, if the model could evaluate more correlated frames before generating an output it would, in our minds, have been able to learn the features of the videos better, this is the main reason why the 2D models proved unsatisfactory. So we decided to try a different approach, the 3D CNN one.